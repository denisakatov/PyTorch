{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Обрабатываем данные"},{"metadata":{"trusted":true},"cell_type":"code","source":"np.unique(plt.imread('../input/makeup-lips-segmentation-28k-samples/set-lipstick-original/mask/mask00000001.png')[:,:,0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(100):\n    mask = plt.imread('../input/makeup-lips-segmentation-28k-samples/set-lipstick-original/mask/mask00000002.png')\n    mask = np.where(mask == i, 255, 0)\n    mask = mask[:,:,0]\n    print(np.unique(mask))\n    plt.imshow(mask)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"image = plt.imread('../input/makeup-lips-segmentation-28k-samples/set-lipstick-original/mask/mask00000002.png')[:,:,:]\nplt.imshow(image)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.imshow(plt.imread('../input/makeup-lips-segmentation-28k-samples/set-lipstick-original/mask/mask00000003.png')[:,:,1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"p720 = []\nmask = []\nfor root, dirs, files in os.walk('../input/makeup-lips-segmentation-28k-samples/set-lipstick-original'):\n    for name in files:\n        f = os.path.join(root, name)\n        if '720p' in f:\n            p720.append(f)\n        elif 'mask' in f:\n            mask.append(f)\n        else:\n            break","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(p720), len(mask)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. Упорядочим и возьмем первые 100 (чтобы быстрее обработать)"},{"metadata":{"trusted":true},"cell_type":"code","source":"p720 = sorted(p720)\nmask = sorted(mask)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"p720, mask = p720[:100], mask[:100]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Посмотрим размеры картинок"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.imread('../input/makeup-lips-segmentation-28k-samples/set-lipstick-original/720p/image00000001.jpg').shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.imread('../input/makeup-lips-segmentation-28k-samples/set-lipstick-original/mask/mask00000001.png').shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new = plt.imread('../input/makeup-lips-segmentation-28k-samples/set-lipstick-original/720p/image00000001.jpg')\nplt.imshow(new)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new = plt.imread('../input/makeup-lips-segmentation-28k-samples/set-lipstick-original/mask/mask00000001.png')[:,:,1]\nplt.imshow(new)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.DataFrame({'p720': p720, 'mask': mask})\n# Отсортируем  датафрейм по значениям\ndf.sort_values(by='p720',inplace=True)\n# Используем функцию,\n# лагодаря которой индексация значений \n# будет начинаться с 0.\ndf.reset_index(drop=True, inplace=True)\n# Выведем первые ять значений нашего датафрейма\nprint(df.head(5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\nimport torch\nfrom torch.nn import functional as F","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class CustomDatasetFromImages(Dataset):\n    def __init__(self, data_info):\n        # Подаем наш подготовленный датафрейм\n        self.data_info = data_info\n        \n        # Разделяем датафрейм на rgb картинки \n        self.image_arr = self.data_info.iloc[:,0]\n        # и на сегментированные картинки\n        self.label_arr = self.data_info.iloc[:,1]\n        \n        # Количество пар картинка-сегментация\n        self.data_len = len(self.data_info.index)\n    def __getitem__(self, index):\n        # Читаем картинку и сразу же представляем ее в виде numpy-массива \n        # размера 600х800 float-значний\n        img = Image.open(self.image_arr[index])\n        img = img.resize((256,256))\n        img = np.asarray(img).astype('float')\n        # Нормализуем изображение в значениях [0,1]\n        img = torch.as_tensor(img)/255    \n        # unsqueeze - меняет размерность img c (600, 800, 3) -> (1, 600, 800, 3),\n        # т.е. оборачивает картинку в батч размером в одну картинку\n        img = img.permute(2,0,1)\n        # Мы используем функцию интерполяции для того,\n        # чтобы поменять рамерность картинки с 800х600 на 256х256\n        #img = F.interpolate(input=img, size=256, align_corners=False, mode='bicubic')\n        \n        \n        # итаем сегментированную картинку и сразу же представляем ее в виде numpy-массива \n        # размера 600х800 float-значний\n        lab = np.asarray(plt.imread(self.label_arr[index]))[:,:,0]*255\n        lab = torch.as_tensor(lab).unsqueeze(0)\n        \n        \n        \n        \n        # делаем ресайз картинки на 256х256\n        # Для этого используем функцию interpolate\n        ### Что бы ресайзить и высоту и ширину картинки, нужно перед interpolate\n        ### пороизвести unsqueeze над тензором, и squeeze после.\n        lab = lab.unsqueeze(0)\n        lab = F.interpolate(input=lab, size=256, mode='nearest')\n        \n        lab=lab.squeeze(0).squeeze(0)\n        \n        \n        \n        return (img.float(), lab.float())\n\n    def __len__(self):\n        return self.data_len","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# 70 % в тренировочную выборку, 30 - в тестовую\nX_train, X_test = train_test_split(df,test_size=0.3)\n\n# Упорядочиваем индексацию\nX_train.reset_index(drop=True,inplace=True)\nX_test.reset_index(drop=True,inplace=True)\n\n# Оборачиваем каждую выборку в наш кастомный датасет\ntrain_data = CustomDatasetFromImages(X_train)\ntest_data = CustomDatasetFromImages(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data_loader = DataLoader(train_data,batch_size=8,shuffle=True)\ntest_data_loader = DataLoader(test_data,batch_size=4,shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Обучение модели"},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass UNet(nn.Module):\n\n    def __init__(self, num_classes):\n        super(UNet, self).__init__()\n        self.num_classes = num_classes\n\n        # Левая сторона (Путь уменьшения размерности картинки)\n        self.down_conv_11 = self.conv_block(in_channels=3, out_channels=64)\n        self.down_conv_12 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.down_conv_21 = self.conv_block(in_channels=64, out_channels=128)\n        self.down_conv_22 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.down_conv_31 = self.conv_block(in_channels=128, out_channels=256)\n        self.down_conv_32 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.down_conv_41 = self.conv_block(in_channels=256, out_channels=512)\n        self.down_conv_42 = nn.MaxPool2d(kernel_size=2, stride=2)\n        \n        self.middle = self.conv_block(in_channels=512, out_channels=1024)\n        \n        # Правая сторона (Путь увеличения размерности картинки)\n        self.up_conv_11 = nn.ConvTranspose2d(in_channels=1024, out_channels=512, kernel_size=3, stride=2, padding=1, output_padding=1)\n        self.up_conv_12 = self.conv_block(in_channels=1024, out_channels=512)\n        self.up_conv_21 = nn.ConvTranspose2d(in_channels=512, out_channels=256, kernel_size=3, stride=2, padding=1, output_padding=1)\n        self.up_conv_22 = self.conv_block(in_channels=512, out_channels=256)\n        self.up_conv_31 = nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=3, stride=2, padding=1, output_padding=1)\n        self.up_conv_32 = self.conv_block(in_channels=256, out_channels=128)\n        self.up_conv_41 = nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=3, stride=2, padding=1, output_padding=1)\n        self.up_conv_42 = self.conv_block(in_channels=128, out_channels=64)\n        \n        self.output = nn.Conv2d(in_channels=64, out_channels=num_classes, kernel_size=3, stride=1, padding=1)\n        self.softmax = nn.Softmax()\n    \n    @staticmethod\n    def conv_block(in_channels, out_channels):\n        block = nn.Sequential(nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1),\n                                    nn.ReLU(),\n                                    nn.BatchNorm2d(num_features=out_channels),\n                                    nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1),\n                                    nn.ReLU(),\n                                    nn.BatchNorm2d(num_features=out_channels))\n        return block\n    \n    @staticmethod\n    def crop_tensor(target_tensor, tensor):\n        target_size = target_tensor.size()[2]\n        tensor_size = tensor.size()[2]\n        delta = tensor_size - target_size\n        delta = delta // 2\n\n        return tensor[:,:, delta:tensor_size-delta, delta:tensor_size-delta]\n\n\n    def forward(self, X):\n        # Проход по левой стороне\n        x1 = self.down_conv_11(X) # [-1, 64, 256, 256]\n        x2 = self.down_conv_12(x1) # [-1, 64, 128, 128]\n        x3 = self.down_conv_21(x2) # [-1, 128, 128, 128]\n        x4 = self.down_conv_22(x3) # [-1, 128, 64, 64]\n        x5 = self.down_conv_31(x4) # [-1, 256, 64, 64]\n        x6 = self.down_conv_32(x5) # [-1, 256, 32, 32]\n        x7 = self.down_conv_41(x6) # [-1, 512, 32, 32]\n        x8 = self.down_conv_42(x7) # [-1, 512, 16, 16]\n        \n        middle_out = self.middle(x8) # [-1, 1024, 16, 16]\n\n        # Проход по правой стороне\n        x = self.up_conv_11(middle_out) # [-1, 512, 32, 32]\n        y = self.crop_tensor(x, x7)\n        x = self.up_conv_12(torch.cat((x, y), dim=1)) # [-1, 1024, 32, 32] -> [-1, 512, 32, 32]\n        \n        x = self.up_conv_21(x) # [-1, 256, 64, 64]\n        y = self.crop_tensor(x, x5)\n        x = self.up_conv_22(torch.cat((x, y), dim=1)) # [-1, 512, 64, 64] -> [-1, 256, 64, 64]\n        \n        x = self.up_conv_31(x) # [-1, 128, 128, 128]\n        y = self.crop_tensor(x, x3)\n        x = self.up_conv_32(torch.cat((x, y), dim=1)) # [-1, 256, 128, 128] -> [-1, 128, 128, 128]\n        \n        x = self.up_conv_41(x) # [-1, 64, 256, 256]\n        y = self.crop_tensor(x, x1)\n        x = self.up_conv_42(torch.cat((x, y), dim=1)) # [-1, 128, 256, 256] -> [-1, 64, 256, 256]\n        \n        output = self.output(x) # [-1, num_classes, 256, 256]\n        output = self.softmax(output)\n\n        return output","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learning_rate = 0.001\nepochs = 5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Umodel = UNet(num_classes=150).to(device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"optimizer = torch.optim.Adam(Umodel.parameters())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def dice_loss(output, target, weights=None, ignore_index=None):\n    \"\"\"\n    output : NxCxHxW Variable\n    target :  NxHxW LongTensor\n    weights : C FloatTensor\n    ignore_index : int index to ignore from loss\n    \"\"\"\n    eps = 0.0001\n\n    output = output.float().exp()\n    target = target.type(torch.int64)\n    encoded_target = output.detach() * 0\n    if ignore_index is not None:\n        mask = target == ignore_index\n        target = target.clone()\n        target[mask] = 0\n        encoded_target.scatter_(1, target.unsqueeze(1), 1)\n        mask = mask.unsqueeze(1).expand_as(encoded_target)\n        encoded_target[mask] = 0\n    else:\n        encoded_target.scatter_(1, target.unsqueeze(1), 1)\n\n    if weights is None:\n        weights = 1\n\n    intersection = output * encoded_target\n    numerator = 2 * intersection.sum(0).sum(1).sum(1)\n    denominator = output + encoded_target\n\n    if ignore_index is not None:\n        denominator[mask] = 0\n    denominator = denominator.sum(0).sum(1).sum(1) + eps\n    loss_per_channel = weights * (1 - (numerator / denominator))\n\n    return loss_per_channel.sum() / output.size(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class SoftDiceLoss(nn.Module):\n    def __init__(self, weight=None, size_average=True):\n        super(SoftDiceLoss, self).__init__()\n\n    def forward(self, logits, targets):\n        smooth =1\n        num = targets.size(0)\n        probs = logits\n        m1 = probs.view(num, -1)\n        m2 = targets.view(num, -1)\n        intersection = (m1 * m2)\n\n        score =(2. * intersection.sum(1) + smooth) / (m1.sum(1) + m2.sum(1) + smooth)\n        #print(score.sum())\n        score =1 - score.sum() / num\n        return score\n    # This function has only a single output, so it gets only one gradient\n    def backward(self, grad_output):\n\n        input, target = self.saved_variables\n        grad_input = grad_target = None\n\n        if self.needs_input_grad[0]:\n            grad_input = grad_output * 2 * (target * self.union - self.inter) \\\n                         / (self.union * self.union)\n        if self.needs_input_grad[1]:\n            grad_target = None\n\n        return grad_input, grad_target","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"total_steps = len(train_data_loader)\nprint(f\"{epochs} epochs, {total_steps} total_steps per epoch\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"criterion=SoftDiceLoss()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Импортируем библиотеку time для расчета, сколько времени у нас уходит на одну эпоху\nimport time\nfrom torch.autograd import Variable\n\n# Полезная функция для детектирования аномалий в процессе обучения\n#torch.autograd.set_detect_anomaly(True)\n\n# запускаем главный тренировочный цикл\nepoch_losses = []\nfor epoch in range(epochs):\n    time1 = time.time()\n    epoch_loss = []\n    for batch_idx, (data, labels) in enumerate(train_data_loader):\n        \n        data, labels = data.to(device), labels.to(device)        \n#         data = data.to(device)\n#         labels = labels.to(device)\n        \n        optimizer.zero_grad()\n        \n        #print(data.shape)\n        #print(labels.shape)\n        \n        outputs = Umodel(data)\n        \n        \n        \n        #print(outputs.shape)\n        \n        \n        #loss = nn.CrossEntropyLoss(outputs,labels)# - torch.log(SoftDiceLoss(outputs, labels))\n        loss = dice_loss(outputs, labels)\n        \n        \n        loss.backward()\n        optimizer.step()\n        \n        epoch_loss.append(loss.item())\n        \n        if batch_idx%200==0:\n            print(f'batch index : {batch_idx} | loss : {loss.item()}')\n\n    print(f'Epoch {epoch+1}, loss: ',np.mean(epoch_loss))\n    time2 = time.time()\n    print(f'Spend time for 1 epoch: {time2-time1} sec')\n    \n    epoch_losses.append(epoch_loss)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"savePATH = './selfLastModel_dice_loss.pth'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"torch.save(Umodel.state_dict(), savePATH)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"net=UNet(150).to(device)\nnet.load_state_dict(torch.load(savePATH))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_orig(image):\n    #image = images[0,:,:,:]\n    image = image.permute(1, 2, 0)\n    image = image.numpy()\n    image = np.clip(image, 0, 1)\n    return image","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i, data in enumerate(test_data_loader):\n    images, labels = data\n    images = images.to(device)\n    labels = labels.to(device)\n    outputs = net(images)\n    f, axarr = plt.subplots(1,3)\n\n    for j in range(0,5):\n        axarr[0].imshow(outputs.squeeze().detach().cpu().numpy()[j,5,:,:])\n        axarr[0].set_title('Guessed labels')\n\n        axarr[1].imshow(labels.detach().cpu().numpy()[j,:,:])\n        axarr[1].set_title('Ground truth labels')\n\n        original = get_orig(images[j].cpu())\n        axarr[2].imshow(original)\n        axarr[2].set_title('Original Images')\n        plt.show()\n        plt.gcf().show()\n        if i>1:\n            break","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}